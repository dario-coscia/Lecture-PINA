{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f01652",
   "metadata": {},
   "outputs": [],
   "source": [
    "## routine needed to run the notebook on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    !pip install \"pina-mathlab[tutorial]\"\n",
    "    !pip install \"ase\"\n",
    "    !mkdir \"data\"\n",
    "    !wget \"https://github.com/dario-coscia/KTH-Summer-School-PINNs-PINA/raw/refs/heads/main/data/ammonia.xyz\" -O \"data/ammonia.xyz\"\n",
    "    !wget \"https://github.com/mathLab/PINA/raw/refs/heads/master/tutorials/tutorial7/data/pinn_solution_0.5_0.5\" -O \"data/pinn_solution_0.5_0.5\"\n",
    "    !wget \"https://github.com/mathLab/PINA/raw/refs/heads/master/tutorials/tutorial7/data/pts_0.5_0.5\" -O \"data/pts_0.5_0.5\"\n",
    "    !mkdir \"benchmark_ckpt\"\n",
    "    !wget \"https://github.com/dario-coscia/KTH-Summer-School-PINNs-PINA/raw/refs/heads/main/benchmark_ckpt/PINN.ckpt\" -O \"benchmark_ckpt/PINN.ckpt\"\n",
    "    !wget \"https://github.com/dario-coscia/KTH-Summer-School-PINNs-PINA/raw/refs/heads/main/benchmark_ckpt/GradientPINN.ckpt\" -O \"benchmark_ckpt/GradientPINN.ckpt\"\n",
    "    !wget \"https://github.com/dario-coscia/KTH-Summer-School-PINNs-PINA/raw/refs/heads/main/benchmark_ckpt/RBAPINN.ckpt\" -O \"benchmark_ckpt/RBAPINN.ckpt\"\n",
    "\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, Callback\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from ase.io import read\n",
    "\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "from pina import Condition, LabelTensor, Trainer\n",
    "from pina.condition import DataCondition\n",
    "from pina.problem import (AbstractProblem, TimeDependentProblem, SpatialProblem,\n",
    "                          InverseProblem)\n",
    "from pina.domain import CartesianDomain\n",
    "from pina.graph import Graph, LabelBatch\n",
    "from pina.solver import (SingleSolverInterface, SupervisedSolverInterface,\n",
    "                         SupervisedSolver, PINN)\n",
    "from pina.model import FeedForward\n",
    "from pina.model.block import PODBlock\n",
    "from pina.operator import grad, laplacian\n",
    "from pina.model.block.message_passing import DeepTensorNetworkBlock\n",
    "from pina.equation import Equation, FixedValue, FixedGradient\n",
    "from pina.solver import PINN, GradientPINN, RBAPINN\n",
    "from pina.optim import TorchOptimizer\n",
    "from pina.loss import LpLoss, LossInterface\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f71ca5c",
   "metadata": {},
   "source": [
    "# PINA Library for Scientific Machine Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dario-coscia/Lecture-PINA/blob/main/tutorial.ipynb)\n",
    "\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src=\"https://raw.githubusercontent.com/mathLab/PINA/master/readme/pina_logo.png\" alt=\"PINA logo\" width=\"90\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "**PINA** [1] is an open-source Python library designed for **Scientific Machine Learning (SciML)** tasks, particularly involving:\n",
    "\n",
    "- **Physics-Informed Neural Networks (PINNs)**\n",
    "- **Neural Operators (NOs)**\n",
    "- **Reduced Order Models (ROMs)**\n",
    "- **Generative Models (GMs)**\n",
    "- ...\n",
    "\n",
    "Built on **PyTorch**, **PyTorch Lightning**, and **PyTorch Geometric**, it provides a **user-friendly, intuitive interface** for formulating and solving differential problems using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796ec17",
   "metadata": {},
   "source": [
    "## üëâ Lecture Outline\n",
    "\n",
    "1. **Introduction to the Software**\n",
    "    - How to build a SciML problem in PINA\n",
    "    - Data Structures and Domains\n",
    "    - Solvers and Trainer API     \n",
    "\n",
    "2. **Supervised Learning**\n",
    "    - Machine Learning Interatomic Potentials (*Ammonia NH3*)\n",
    "    - Reduced Order Model learning (*Navier Stokes back-step*)\n",
    "\n",
    "3. **Physics Informed Learning**\n",
    "    - Train Physics Informed Networks with PINA (*Harmonic Oscillator*)\n",
    "    - Benchmarking PINN variants (*Helmholtz Equation*)\n",
    "    - Solving Inverse Problems (*Poisson Equation*)\n",
    "\n",
    "> üôÅ We will not cover Neural Operators, but feel free to check the [tutorials](https://github.com/mathLab/PINA/tree/master/tutorials#neural-operator-learning) on the PINA repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d85bd",
   "metadata": {},
   "source": [
    "## üëâ Introduction to the Software\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"http://raw.githubusercontent.com/mathLab/PINA/master/tutorials/static/pina_workflow.png\" alt=\"PINA Workflow\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "**PINA** is an open-source Python library designed to simplify and accelerate the development of SciML solutions. Built on top of [PyTorch](https://pytorch.org/), [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), and [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), PINA provides an intuitive framework based of four main steps:\n",
    "\n",
    "1. ***Problem & Data***\n",
    "   Define the mathematical problem and its physical constraints +  Prepare inputs by discretizing the domain or importing numerical data. \n",
    "\n",
    "2. ***Model Design***  \n",
    "   Build neural network models as **PyTorch modules**. For graph-structured data, use **PyTorch Geometric** to build Graph Neural Networks. You can also import models from `pina.model` module!\n",
    "\n",
    "3. ***Solver Selection***  \n",
    "   Choose and configure a solver to optimize your model, e.g. `SupervisedSolver`, `ReducedOrderModelSolver`, `PINN` and more...\n",
    "\n",
    "4. ***Training***  \n",
    "   Train your model using the `Trainer` class (built on **PyTorch Lightning**), which enables scalable and efficient training with advanced features.\n",
    "\n",
    "Let's see more in details all the steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014129d",
   "metadata": {},
   "source": [
    "### 1. Problem & Data\n",
    "In **PINA**, defining a problem is done by creating a Python `class` that inherits from one or more problem classes, such as `SpatialProblem`, `TimeDependentProblem`, or `ParametricProblem`, depending on the nature of the problem. A simple example is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating some fictitious data\n",
    "input_1 = LabelTensor(torch.randn(10, 1), \"x\")  # <== input_variables\n",
    "input_2 = LabelTensor(torch.randn(10, 1), \"x\")  # <== input_variables\n",
    "target_1 = LabelTensor(torch.randn(10, 1), \"y\")  # <== output_variables\n",
    "target_2 = LabelTensor(torch.randn(10, 1), \"y\")  # <== output_variables\n",
    "\n",
    "\n",
    "class MySupervisedProblem(AbstractProblem):\n",
    "\n",
    "    input_variables = [\"x\"]\n",
    "    output_variables = [\"y\"]\n",
    "\n",
    "    conditions = {\n",
    "        \"condition_1\": Condition(input=input_1, target=target_1),\n",
    "        \"condition_2\": Condition(input=input_2, target=target_2),\n",
    "    }\n",
    "\n",
    "\n",
    "problem = MySupervisedProblem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1e4c4",
   "metadata": {},
   "source": [
    "We highlight two very important features of PINA\n",
    "\n",
    "1. **`Condition` Object**  \n",
    "   - The `Condition` object enforces the **constraints** that the model must satisfy, such as boundary or initial conditions. More conditions will come in the tutorial!\n",
    "   - It ensures that the model adheres to the specific requirements of the problem, making constraint handling more intuitive and streamlined.\n",
    "\n",
    "2. **`LabelTensor` Structure**  \n",
    "   - Alongside the standard `torch.Tensor`, PINA introduces the `LabelTensor` structure, which allows **string-based indexing**.  \n",
    "   - Ideal for managing and stacking tensors with different labels (e.g., `\"x\"`, `\"t\"`, `\"u\"`) for improved clarity and organization.  \n",
    "   - You can still use standard PyTorch tensors if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label tensor containing spatial and temporal coordinates\n",
    "x = torch.tensor([0.0, 1.0, 2.0, 3.0])  # Spatial coordinates\n",
    "t = torch.tensor([0.0, 0.5, 1.0, 1.5])  # Time coordinates\n",
    "\n",
    "# Combine x and t into a label tensor (2D tensor)\n",
    "tensor = torch.stack([x, t], dim=-1)  # Shape: [4, 2]\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "\n",
    "# Build the LabelTensor\n",
    "label_tensor = LabelTensor(tensor, [\"x\", \"t\"])\n",
    "\n",
    "print(f\"Torch methods can be used, {label_tensor.shape=}\")\n",
    "print(f\"also {label_tensor.requires_grad=} \\n\")\n",
    "print(f'We can slice with labels: \\n {label_tensor[\"x\"]=}')\n",
    "print(f\"Similarly to: \\n {label_tensor[:, 0]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba378a7",
   "metadata": {},
   "source": [
    "You can do more complex slicing by using the extract method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14386b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = LabelTensor(\n",
    "    tensor,\n",
    "    {\n",
    "        0: {\"dof\": range(4), \"name\": \"points\"},\n",
    "        1: {\"dof\": [\"x\", \"t\"], \"name\": \"coords\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f'Extract labels: {label_tensor.extract({\"points\" : [0, 2]})=}')\n",
    "print(f\"Similar to: {label_tensor[slice(0, 4, 2), :]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71961d33",
   "metadata": {},
   "source": [
    "Similarly, you can also work with graphs!\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/dario-coscia/KTH-Summer-School-PINNs-PINA/main/imgs/simple_graph.png\" alt=\"Graph\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node features: [2 nodes, 3 features]\n",
    "x = torch.randn(2, 3)\n",
    "# Edge indices: representing a graph with two edges (node 0 to node 1, node 1 to node 0)\n",
    "edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
    "# Create a PINA graph object (similar to PyG)\n",
    "data = Graph(x=x, edge_index=edge_index)\n",
    "output = torch.randn(10)\n",
    "\n",
    "\n",
    "# create problem\n",
    "class MySupervisedProblem(AbstractProblem):\n",
    "    input_variables = None\n",
    "    output_variables = None\n",
    "    conditions = {\"condition\": Condition(input=data, target=output)}\n",
    "\n",
    "\n",
    "problem = MySupervisedProblem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655181a",
   "metadata": {},
   "source": [
    "### 2. Model Design\n",
    "\n",
    "In PINA we refer to the `model` as the object that solves the problem, e.g., a **Neural Network**. You can implement any neural network using the PyTorch `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2faf776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # <=== always remember this\n",
    "        self.complex_net = nn.Sequential(\n",
    "            nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.complex_net(x)\n",
    "\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce1ae2",
   "metadata": {},
   "source": [
    "or use the PINA built in models in the `pina.model` module, which you can explore further [here](https://mathlab.github.io/PINA/_rst/_code.html#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForward(\n",
    "    input_dimensions=10,\n",
    "    output_dimensions=10,\n",
    "    inner_size=20,\n",
    "    n_layers=1,\n",
    "    func=nn.ReLU,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed2d77",
   "metadata": {},
   "source": [
    "or you can use building blocks from `pina.model.block` to assemble Neural Network layers and build a model ([more in here](https://mathlab.github.io/PINA/_rst/_code.html#blocks))\n",
    "\n",
    "> ‚ö†Ô∏è *Also for message passing networks we have built in blocks, but in `pina.model.messagepassing`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.model.block import ResidualBlock\n",
    "\n",
    "model = nn.Sequential(\n",
    "    ResidualBlock(input_dim=10, output_dim=20, hidden_dim=20),\n",
    "    nn.ReLU(),\n",
    "    ResidualBlock(input_dim=20, output_dim=20, hidden_dim=10),\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c25aad",
   "metadata": {},
   "source": [
    "### 3. Solver Selection\n",
    "\n",
    "[`Solvers`](https://mathlab.github.io/PINA/_rst/_code.html#solvers) are versatile objects in PINA designed to manage the training and optimization of machine learning models. They are built on top of the [PyTorch Lightning `LightningModule`](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), and handle key components of the learning process, including:\n",
    "\n",
    "- Loss function minimization  \n",
    "- Model optimization (optimizer, schedulers)\n",
    "- Validation and testing workflows\n",
    "\n",
    "**Solvers in PINA are modular and hierarchical!** \n",
    "\n",
    "| Feature / Argument | [`SingleSolverInterface`](https://mathlab.github.io/PINA/_rst/solver/solver_interface.html) | [`MultiSolverInterface`](https://mathlab.github.io/PINA/_rst/solver/multi_solver_interface.html) |\n",
    "|--------------------|----------------------------------------------------------|------------------------------------------------------------|\n",
    "| Purpose        | Solvers for **a single model** (e.g., supervised learning, PINNs) | Solvers for **multiple models** (e.g., GANs, ensembles)     |\n",
    "| Inheriting Classes | `SupervisedSolver`, `PINN`, etc.                        | `DeepEnsemblePINN`, `GAROM`, etc.                           |\n",
    "| problem        | ‚úÖ Required                                                | ‚úÖ Required                                                  |\n",
    "| model(s)       | `model` ‚Äì Single model                                    | `models` ‚Äì One or more models                               |\n",
    "| optimizer(s)   | `optimizer` ‚Äì Defaults to `torch.optim.Adam`              | `optimizers` ‚Äì Defaults to `torch.optim.Adam`               |\n",
    "| scheduler(s)   | `scheduler` ‚Äì Defaults to `torch.optim.lr_scheduler.ConstantLR` | `schedulers` ‚Äì Defaults to `torch.optim.lr_scheduler.ConstantLR` |\n",
    "| weighting(s)   | Optional, [see docs](https://mathlab.github.io/PINA/_rst/_code.html#losses-and-weightings) | Optional, [see docs](https://mathlab.github.io/PINA/_rst/_code.html#losses-and-weightings) |\n",
    "| **use_lt**         | Whether to use LabelTensors as input                      | Whether to use LabelTensors as input                        |\n",
    "\n",
    "\n",
    "**These classes are used to define the backbone**, i.e. setting the problem, the model(s), the optimizer(s) and scheduler(s), but miss a key component the `optimization_cycle` method. The **`optimization_cycle`** method is the core function responsible for computing losses for **all conditions** in a given training batch. Each condition (e.g. initial condition, boundary condition, PDE residual) contributes its own loss, which is tracked and returned in a dictionary. This method should return a dictionary mapping **condition names** to their respective **scalar loss values**.\n",
    "\n",
    "For supervised learning tasks, where each condition consists of an input-target pair, for example, the `optimization_cycle` may look like this:\n",
    "\n",
    "```python\n",
    "def optimization_cycle(self, batch):\n",
    "    \"\"\"\n",
    "    The optimization cycle for Supervised solvers.\n",
    "    Computes loss for each condition in the batch.\n",
    "    \"\"\"\n",
    "    condition_loss = {}\n",
    "    for condition_name, data in batch:\n",
    "        condition_loss[condition_name] = self.loss_data(\n",
    "            input=data[\"input\"], target=data[\"target\"]\n",
    "        )\n",
    "    return condition_loss\n",
    "```\n",
    "In PINA, a **batch** is structured as a list of tuples, where each tuple corresponds to a specific training condition. Each tuple contains:\n",
    "\n",
    "- The **name of the condition**\n",
    "- A **dictionary of data** associated with that condition\n",
    "\n",
    "for example:\n",
    "\n",
    "```python\n",
    "batch = [\n",
    "    (\"condition1\", {\"input\": ..., \"target\": ...}),\n",
    "    (\"condition2\", {\"input\": ..., \"equation\": ...}),\n",
    "    (\"condition3\", {\"input\": ..., \"target\": ...}),\n",
    "]\n",
    "```\n",
    "\n",
    "Fortunately, you don't need to implement the `optimization_cycle` yourself in most cases ‚Äî PINA already provides default implementations tailored to common solver types. These implementations are available through the solver interfaces and cover various training strategies.\n",
    "\n",
    "1. [`PINNInterface`](https://mathlab.github.io/PINA/_rst/solver/physics_informed_solver/pinn_interface.html)  \n",
    "   Implements the optimization cycle for **physics-based solvers** (e.g., PDE residual minimization) as well as other useful methods to compute PDE residuals.  \n",
    "   ‚û§ [View method](https://mathlab.github.io/PINA/_rst/solver/physics_informed_solver/pinn_interface.html#pina.solver.physics_informed_solver.pinn_interface.PINNInterface.optimization_cycle)\n",
    "\n",
    "2. [`SupervisedSolverInterface`](https://mathlab.github.io/PINA/_rst/solver/supervised_solver/supervised_solver_interface.html)  \n",
    "   Defines the optimization cycle for **supervised learning tasks**, including traditional regression and classification.  \n",
    "   ‚û§ [View method](https://mathlab.github.io/PINA/_rst/solver/supervised_solver/supervised_solver_interface.html#pina.solver.supervised_solver.supervised_solver_interface.SupervisedSolverInterface.optimization_cycle)\n",
    "\n",
    "3. [`DeepEnsembleSolverInterface`](https://mathlab.github.io/PINA/_rst/solver/ensemble_solver/ensemble_solver_interface.html)  \n",
    "   Provides the optimization logic for **deep ensemble methods**, commonly used for uncertainty quantification or robustness.  \n",
    "   ‚û§ [View method](https://mathlab.github.io/PINA/_rst/solver/ensemble_solver/ensemble_solver_interface.html#pina.solver.ensemble_solver.ensemble_solver_interface.DeepEnsembleSolverInterface.optimization_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstSolver(SupervisedSolverInterface, SingleSolverInterface):\n",
    "\n",
    "    # loss data is responsible for \"data driven loss\"\n",
    "    # for physics based learning you need to implement\n",
    "    # loss_phys for building custom physics informed solvers\n",
    "    def loss_data(self, input, target):\n",
    "        network_output = self.forward(input)\n",
    "        return self.loss(network_output, target)\n",
    "\n",
    "\n",
    "# initialize (we use plain tensors!)\n",
    "solver = MyFirstSolver(problem=MySupervisedProblem(), model=SimpleNet(), use_lt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cba096",
   "metadata": {},
   "source": [
    "#### 4. Training\n",
    "\n",
    "Finally the `Trainer` class is used to train the model. The `Trainer` class, based on **PyTorch Lightning**, offers many features that help:\n",
    "- **Improve model accuracy**\n",
    "- **Reduce training time and memory usage**\n",
    "- **Facilitate logging and visualization**  \n",
    "\n",
    "The great work done by the PyTorch Lightning team ensures a streamlined training process, we will see more in details some features later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8b2cb",
   "metadata": {},
   "source": [
    "## üëâ Supervised Learning\n",
    "\n",
    "**Supervised learning** is a machine learning paradigm where a model $\\mathcal{M}_{\\theta}$ is trained on a **labeled** dataset. In this setting, the goal is to learn a mapping from inputs $x$ to outputs $y$, using a dataset of known input-output pairs $\\{x_i, y_i\\}_{i=1}^N$. The model adjusts its internal parameters to minimize the discrepancy between its predictions and the actual target values, quantified by a loss function: $$\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^N l(y_i, \\mathcal{M}_{\\theta}(x_i)).$$\n",
    "\n",
    "We will now show how to apply what you have learned so far for two problems:\n",
    "\n",
    "2. Machine Learned Interatomic Potential (*Ammonia System*)\n",
    "2. Reduced Order Model learning (*Navier Stokes backstep*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c5b3f",
   "metadata": {},
   "source": [
    "### Machine Learning Interatomic Potentials\n",
    "\n",
    "Machine Learning Interatomic Potentials (MLIPs) [7] are models that learn to approximate the **potential energy surface (PES)** of atomic systems using data from quantum mechanical simulations.\n",
    "\n",
    "The core idea is to **regress the total energy** $E$ of a system given atomic positions $\\boldsymbol{r}\\in\\mathbb{R}^3$ and atomic numbers $Z\\in\\mathbb{R}$ ‚Äî and then obtain the **atomic forces** $F$ as the **negative gradient of the predicted energy** with respect to atomic positions:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} E(\\{\\mathbf{r}_j\\})\n",
    "$$\n",
    "\n",
    "This makes the problem naturally **supervised**:  \n",
    "We train a model to predict both:\n",
    "\n",
    "- **Energies**: scalar value per structure  \n",
    "- **Forces**: vector (3D) per atom, derived from energy via automatic differentiation\n",
    "\n",
    "These models are used in **molecular dynamics**, **geometry optimization**, and **materials discovery**, where accurate yet fast energy and force evaluations are critical. We first divide in train and test data, we will use the `Data` structure as it naturally handles graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read trajectory from .xyz\n",
    "atoms_list = read(\"data/ammonia.xyz\", index=\":\")  # list of ase.Atoms\n",
    "random.shuffle(atoms_list)\n",
    "\n",
    "# Divide train and test\n",
    "train_atoms = atoms_list[:140]\n",
    "test_atoms = atoms_list[140:]\n",
    "\n",
    "# Build input_data and output_data for training\n",
    "input_data = []\n",
    "output_data = []\n",
    "for atoms in train_atoms:\n",
    "    pos = LabelTensor(\n",
    "        torch.tensor(atoms.get_positions(), dtype=torch.float32),\n",
    "        [\"x\", \"y\", \"z\"],\n",
    "    )\n",
    "    z = torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long)\n",
    "    energy = torch.tensor([atoms.get_potential_energy()], dtype=torch.float32)\n",
    "    forces = torch.tensor(atoms.get_forces(), dtype=torch.float32)\n",
    "    input_data.append(Graph(pos=pos, z=z))\n",
    "    output_data.append(Graph(energy=energy, forces=forces))\n",
    "\n",
    "\n",
    "# general supervised problems, work for any data-structure: Data, LabelTensor,...\n",
    "class SupervisedProblem(AbstractProblem):\n",
    "    conditions = {}\n",
    "    output_variables = None\n",
    "    input_variables = None\n",
    "\n",
    "    def __init__(self, input_, output_, input_variables=None, output_variables=None):\n",
    "        # Set input and output variables\n",
    "        self.input_variables = input_variables\n",
    "        self.output_variables = output_variables\n",
    "        # Set the condition\n",
    "        self.conditions[\"data\"] = Condition(input=input_, target=output_)\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "# Create the problem (we save energy meand and std for later)\n",
    "energy_mean = torch.stack([d.energy for d in output_data]).mean()\n",
    "energy_std = torch.stack([d.energy for d in output_data]).std()\n",
    "problem = SupervisedProblem(input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d801a",
   "metadata": {},
   "source": [
    "We also build similary the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building input_data and output_data for testing\n",
    "input_batch = LabelBatch.from_data_list(\n",
    "    [\n",
    "        Graph(\n",
    "            pos=LabelTensor(\n",
    "                torch.tensor(atoms.get_positions(), dtype=torch.float32),\n",
    "                [\"x\", \"y\", \"z\"],\n",
    "            ),\n",
    "            z=torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long),\n",
    "        )\n",
    "        for atoms in test_atoms\n",
    "    ],\n",
    ")\n",
    "true_energies = torch.stack(\n",
    "    [\n",
    "        torch.tensor(atoms.get_potential_energy(), dtype=torch.float32)\n",
    "        for atoms in test_atoms\n",
    "    ]\n",
    ").flatten()\n",
    "true_forces = torch.stack(\n",
    "    [torch.tensor(atoms.get_forces(), dtype=torch.float32) for atoms in test_atoms]\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58dc81",
   "metadata": {},
   "source": [
    "We will use **Deep Tensor Graph Neural Networks** (DT-GNNs), a class of models designed to respect the physical symmetries of the problem. In particular, these networks are **invariant under translations and rotations**, which is essential when modeling atomic systems where absolute positions should not affect the predictions.\n",
    "\n",
    "The message-passing update for each atom $i$ at layer $l+1$ takes the form:\n",
    "\n",
    "$$\n",
    "h_i^{(l+1)} = h_i^{(l)} + \\sum_{j \\ne i} \\gamma\\left( \\phi(h_i^{(l)}) \\cdot \\psi(\\| \\mathbf{r}_{ij} \\|) \\right)\n",
    "$$\n",
    "\n",
    "- $h_i^{(l)}$: feature vector of atom $i$ at layer $l$ (for first layer we embed $Z$)  \n",
    "- $\\| \\mathbf{r}_{ij} \\|$: distance between atoms $i$ and $j$.  \n",
    "- $\\phi, \\psi, \\gamma$: learnable neural network functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForceField(nn.Module):\n",
    "\n",
    "    def __init__(self, energy_mean, energy_std, cutoff=5):\n",
    "        super().__init__()\n",
    "        self.mean = energy_mean\n",
    "        self.std = energy_std\n",
    "        self.cutoff = cutoff\n",
    "        self.embedding = nn.Embedding(100, 64)\n",
    "        self.linear = nn.Linear(1, 64)\n",
    "        self.gnn1 = DeepTensorNetworkBlock(64, 64)\n",
    "        self.gnn2 = DeepTensorNetworkBlock(64, 64)\n",
    "        self.readout = nn.Sequential(nn.Linear(64, 64), nn.SiLU(), nn.Linear(64, 1))\n",
    "\n",
    "    @torch.set_grad_enabled(\n",
    "        True\n",
    "    )  # we set it to true because we need to compute gradients of forces\n",
    "    def forward(self, data):\n",
    "        pos, z, batch = data.pos, data.z, data.batch\n",
    "        pos = pos.requires_grad_()\n",
    "        edge_index, dist = self.build_edges(pos, batch)\n",
    "        h = self.embedding(z)\n",
    "        edge_attr = self.linear(dist)\n",
    "        # egnn\n",
    "        h = self.gnn1(h, edge_index, edge_attr)\n",
    "        h = self.gnn2(h, edge_index, edge_attr)\n",
    "        # redout\n",
    "        r = self.readout(h)\n",
    "        # compute energy\n",
    "        energy = scatter(r, batch, dim=0, reduce=\"sum\") * self.std + self.mean\n",
    "        predicted_energy = LabelTensor(energy, \"E\")\n",
    "        # compute forces\n",
    "        predicted_forces = -grad(\n",
    "            predicted_energy, pos, d=[\"x\", \"y\", \"z\"], components=[\"E\"]\n",
    "        )\n",
    "        return predicted_energy.tensor, predicted_forces.tensor  # return tensors\n",
    "\n",
    "    def build_edges(self, pos, batch):\n",
    "        edge_index_list, edge_attr_list = [], []\n",
    "        for i in batch.unique():\n",
    "            idx = (batch == i).nonzero(as_tuple=True)[0]\n",
    "            pos_i = pos[idx].tensor\n",
    "            rel = pos_i[:, None, :] - pos_i[None, :, :]\n",
    "            dist = torch.norm(rel, dim=-1)\n",
    "            mask = (dist < self.cutoff) & (dist > 0)\n",
    "            row, col = mask.nonzero(as_tuple=True)\n",
    "            edge_index = torch.stack([idx[row], idx[col]], dim=0)\n",
    "            edge_attr = dist[mask].unsqueeze(1)\n",
    "            edge_index_list.append(edge_index)\n",
    "            edge_attr_list.append(edge_attr)\n",
    "        return torch.cat(edge_index_list, dim=1), torch.cat(edge_attr_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b540630",
   "metadata": {},
   "source": [
    "Once we have built the network, we will define a **custom loss** for the solver to train it. To do this, we will use the `LossInterface` class. These interfaces allow us to customize the loss function while benefiting from existing infrastructure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForceFieldLoss(LossInterface):\n",
    "    def forward(self, input, target):\n",
    "        predicted_energy, predicted_forces = input\n",
    "        target_energy, target_forces = target.energy, target.forces\n",
    "        energy_loss = torch.abs(target_energy - predicted_energy).mean()\n",
    "        forces_loss = torch.abs(target_forces - predicted_forces).mean()\n",
    "        return energy_loss + forces_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880853e6",
   "metadata": {},
   "source": [
    "We can now train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd40469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ForceField(energy_mean=energy_mean, energy_std=energy_std)\n",
    "solver = SupervisedSolver(problem, model, loss=ForceFieldLoss(), use_lt=False)\n",
    "trainer = Trainer(\n",
    "    solver=solver,\n",
    "    max_epochs=300,\n",
    "    batch_size=32,\n",
    "    accelerator=\"cpu\",\n",
    "    train_size=0.8,\n",
    "    val_size=0.2,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e40be",
   "metadata": {},
   "source": [
    "Finally we can plot the prediction on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69432b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_energies, predicted_forces = solver(input_batch)\n",
    "\n",
    "# Ensure tensors are on CPU and detached from graph\n",
    "pred_energies = predicted_energies.flatten().detach()\n",
    "pred_forces = predicted_forces.flatten().detach()\n",
    "\n",
    "# Create figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Energy plot ---\n",
    "axs[0].scatter(pred_energies, true_energies, alpha=0.6)\n",
    "axs[0].plot(\n",
    "    [true_energies.min(), true_energies.max()],\n",
    "    [true_energies.min(), true_energies.max()],\n",
    "    \"r--\",\n",
    "    label=\"Ideal\",\n",
    ")\n",
    "axs[0].set_xlabel(\"Predicted Energy\")\n",
    "axs[0].set_ylabel(\"True Energy\")\n",
    "axs[0].set_title(\"Energy Prediction\")\n",
    "axs[0].legend()\n",
    "\n",
    "# --- Force plot ---\n",
    "axs[1].scatter(pred_forces, true_forces, alpha=0.3, s=10)\n",
    "axs[1].plot(\n",
    "    [true_forces.min(), true_forces.max()],\n",
    "    [true_forces.min(), true_forces.max()],\n",
    "    \"r--\",\n",
    "    label=\"Ideal\",\n",
    ")\n",
    "axs[1].set_xlabel(\"Predicted Force Component\")\n",
    "axs[1].set_ylabel(\"True Force Component\")\n",
    "axs[1].set_title(\"Force Prediction\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc73d63",
   "metadata": {},
   "source": [
    "### Reduced Order Modelling\n",
    "\n",
    "Reduced Order Modeling (ROM) techniques aim to approximate the solution of complex differential equations‚Äîparticularly **parametric partial differential equations (PDEs)**‚Äîin a fast and efficient way, often enabling **real-time simulation** [2].\n",
    "\n",
    "In this session, we will explore a popular ROM approach known as **POD-NN**, which combines:\n",
    "\n",
    "- **Proper Orthogonal Decomposition (POD)**: for reducing the dimensionality of the solution space (i.e., extracting dominant modes or features), and  \n",
    "- **Neural Networks (NN)**: for learning a nonlinear mapping from parameters to the reduced solution space [3].\n",
    "\n",
    "Specifically, we will:\n",
    "1. Use POD to construct a low-dimensional basis from high-fidelity solution snapshots.\n",
    "2. Train a simple **Multilayer Perceptron (MLP)** to predict the reduced coefficients given new parameter inputs.\n",
    "3. Reconstruct the full solution using the reduced basis and predicted coefficients.\n",
    "\n",
    "> ‚ÑπÔ∏è Although we use an MLP here for simplicity, the approach is flexible and can accommodate other machine learning models as well.\n",
    "\n",
    "Let‚Äôs dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce86f6",
   "metadata": {},
   "source": [
    "We utilize the [Smithers](https://github.com/mathLab/Smithers) library to gather the parametric snapshots (more on the dataset specific can be found in the library). Specifically, we use the `NavierStokesDataset` class, which contains a collection of parametric solutions to the Navier-Stokes equations in a 2D L-shaped domain. The parameter in this case is the inflow velocity.\n",
    "\n",
    "The dataset comprises 500 snapshots of the velocity fields (along the $x$, $y$ axes, and the magnitude), as well as the pressure fields, along with their corresponding parameter values.\n",
    "\n",
    "To visually inspect the snapshots, let's also plot the data points alongside the reference solution. This reference solution represents the expected output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smithers.dataset import NavierStokesDataset\n",
    "\n",
    "dataset = NavierStokesDataset()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(14, 3))\n",
    "for ax, p, u in zip(axs, dataset.params[:4], dataset.snapshots[\"mag(v)\"][:4]):\n",
    "    ax.tricontourf(dataset.triang, u, levels=80)\n",
    "    ax.set_title(rf\"$\\mu$ = {p[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87bdb1",
   "metadata": {},
   "source": [
    "We aim to regress the magnitude of the velocity field $\\|\\boldsymbol{v}\\|$ for different values of the inlet velocity $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa477ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the problem\n",
    "u = torch.tensor(dataset.snapshots[\"mag(v)\"], dtype=torch.float32)\n",
    "p = torch.tensor(dataset.params, dtype=torch.float32)\n",
    "\n",
    "problem = SupervisedProblem(p, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25d4f5",
   "metadata": {},
   "source": [
    "We will now build the model. We highlight that the POD modes are directly computed by means of the singular value decomposition (SVD) over the input data, and not trained using the backpropagation approach. Only the weights of the regressor (MLP) are actually trained during the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2384b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "class PODNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_feautures, pod_rank, num_layers=2, hidden_dim=64, func=nn.SiLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pod = PODBlock(pod_rank)\n",
    "        self.nn = FeedForward(\n",
    "            input_dimensions=in_feautures,\n",
    "            output_dimensions=pod_rank,\n",
    "            n_layers=num_layers,\n",
    "            inner_size=hidden_dim,\n",
    "            func=func,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        coefficents = self.nn(x)  # 1. compute reduced coefficient\n",
    "        return self.pod.expand(coefficents)  # 2. expand the POD basis\n",
    "\n",
    "    def fit_pod(self, x):\n",
    "        self.pod.fit(x)\n",
    "\n",
    "\n",
    "pod_nn_model = PODNN(in_feautures=1, pod_rank=20)\n",
    "\n",
    "# build the solver\n",
    "solver = SupervisedSolver(problem, pod_nn_model, use_lt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a9180",
   "metadata": {},
   "source": [
    "Before starting, we need to fit the POD basis on the training dataset. This can be easily done in **PINA** as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    solver=solver,\n",
    "    max_epochs=500,\n",
    "    batch_size=None,\n",
    "    accelerator=\"cpu\",\n",
    "    train_size=0.8,\n",
    "    val_size=0.1,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# fit the pod basis\n",
    "trainer.data_module.setup(\"fit\")  # set up the dataset\n",
    "train_data = trainer.data_module.train_dataset.get_all_data()\n",
    "x_train = train_data[\"data\"][\"target\"]  # extract data for training\n",
    "pod_nn_model.fit_pod(x=x_train)\n",
    "\n",
    "# now train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c8cb2",
   "metadata": {},
   "source": [
    "Done! Now that the computationally expensive part is over, we can load the model in the future to infer new parameters (simply by loading the checkpoint file automatically created by `Lightning`, see the directory `lightning_logs/...`) or test its performances. We measure the relative error for both the training and test datasets, printing the mean error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0619fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract train and test data\n",
    "trainer.data_module.setup(\"test\")  # set up the dataset\n",
    "p_train = trainer.data_module.train_dataset.conditions_dict[\"data\"][\"input\"]\n",
    "u_train = trainer.data_module.train_dataset.conditions_dict[\"data\"][\"target\"]\n",
    "p_test = trainer.data_module.test_dataset.conditions_dict[\"data\"][\"input\"]\n",
    "u_test = trainer.data_module.test_dataset.conditions_dict[\"data\"][\"target\"]\n",
    "\n",
    "# compute statistics\n",
    "u_test_nn = solver(p_test)\n",
    "u_train_nn = solver(p_train)\n",
    "\n",
    "relative_error_train = torch.norm(u_train_nn - u_train) / torch.norm(u_train)\n",
    "relative_error_test = torch.norm(u_test_nn - u_test) / torch.norm(u_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "param_idx = 0\n",
    "axs[0].set_title(f\"Real Snapshots\")\n",
    "axs[1].set_title(f\"POD-NN\")\n",
    "axs[2].set_title(f\"Error POD-NN\")\n",
    "cm = axs[0].tricontourf(dataset.triang, u_test[param_idx].detach(), levels=80)\n",
    "plt.colorbar(cm, ax=axs[0])\n",
    "cm = axs[1].tricontourf(dataset.triang, u_test_nn[param_idx].detach(), levels=80)\n",
    "plt.colorbar(cm, ax=axs[1])\n",
    "cm = axs[2].tricontourf(\n",
    "    dataset.triang,\n",
    "    (u_test_nn[param_idx] - u_test[param_idx]).abs().detach(),\n",
    "    levels=80,\n",
    ")\n",
    "plt.colorbar(cm, ax=axs[2])\n",
    "plt.show()\n",
    "\n",
    "print(\"Error summary for POD-NN model:\")\n",
    "print(f\"  Train: {relative_error_train.item():.4%}\")\n",
    "print(f\"  Test:  {relative_error_test.item():.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a742f1d",
   "metadata": {},
   "source": [
    "## üëâ Physics Informed Learning\n",
    "\n",
    "**Physics Informed Learning** [4] is a machine learning paradigm where a model $\\mathcal{M}_{\\theta}$ is trained not only on data but also by incorporating known physical laws, typically expressed as differential equations. In this framework, the loss function is augmented with terms derived from physical constraints, such as:\n",
    "\n",
    "- Residuals of partial differential equations (PDEs)\n",
    "- Boundary or initial conditions\n",
    "- Conservation laws (e.g., mass, energy, momentum)\n",
    "\n",
    "For example, in **Physics-Informed Neural Networks (PINNs)**, the total loss $\\mathcal{L}$ typically includes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\lambda_{\\text{data}} \\mathcal{L}_{\\text{data}} + \\lambda_{\\text{physics}} \\mathcal{L}_{\\text{physics}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{L}_{\\text{data}}$ measures the error on observed data,\n",
    "- $\\mathcal{L}_{\\text{physics}}$ penalizes violation of physical laws,\n",
    "- $\\lambda_{\\text{physics}, \\lambda_{\\text{data}}}$ are a weighting coefficients.\n",
    "\n",
    "We will now show how to apply PINA for PINNs problem, specifically:\n",
    "\n",
    "1. Train Physics Informed Networks with PINA (*Harmonic Oscillator*)\n",
    "2. Benchmarking PINN variants (*Advection Equation*)\n",
    "3. Solving Inverse Problems (*Poisson Equation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c334187",
   "metadata": {},
   "source": [
    "### Solving the Harmonic Oscillator with PINNs\n",
    "\n",
    "The harmonic oscillator can be described by a simple ODE:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "&\\frac{d^2}{dt^2}u(t) +  u(t) = 0 \\quad t\\in(0,2\\pi)\\\\\n",
    "&u(t=0) = 0 \\\\\n",
    "&u'(t=0) = 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Although this is a relatively simple problem, we will use it to demonstrate the full pipeline for training a **Physics-Informed Neural Network (PINN)** from scratch using the **PINA** library following the standard 4 steps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the equation as a simple python function\n",
    "def ode_equation(input_, output_):\n",
    "    u_tt = laplacian(output_, input_, components=[\"u\"], d=[\"t\"])\n",
    "    u = output_.extract([\"u\"])\n",
    "    return u_tt + u\n",
    "\n",
    "\n",
    "class HarmonicOscillator(TimeDependentProblem):\n",
    "\n",
    "    # input_variables directly inferred from temporal_domain\n",
    "    output_variables = [\"u\"]\n",
    "    temporal_domain = CartesianDomain({\"t\": [0, 2 * torch.pi]})\n",
    "    domains = {\n",
    "        \"initial\": CartesianDomain({\"t\": 0.0}),\n",
    "        \"pde\": temporal_domain,\n",
    "    }\n",
    "\n",
    "    # one condition for each equation\n",
    "    conditions = {\n",
    "        \"initial\": Condition(domain=\"initial\", equation=FixedValue(0.0)),\n",
    "        \"initial_vel\": Condition(\n",
    "            domain=\"initial\", equation=FixedGradient(1.0, \"u\", \"t\")\n",
    "        ),\n",
    "        \"pde\": Condition(domain=\"pde\", equation=Equation(ode_equation)),\n",
    "        # you can even pass your points!\n",
    "    }\n",
    "\n",
    "    def solution(self, pts):\n",
    "        return torch.sin(pts.extract([\"t\"]))\n",
    "\n",
    "\n",
    "harmonic_oscillator_problem = HarmonicOscillator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78ffc4",
   "metadata": {},
   "source": [
    "Differently from data-driven problems, differential-problems need to specify the domain type. If you look at our ODE definition, the temporal varibale $t$ is defined in the interval $(0,2\\pi)$, and accordingly the `temporal_domain` is a `CartesianDomain` (segment) with the input variable `t` in `[0,2*torch.pi]`. To know more about the Domain class see the [related tutorial](https://mathlab.github.io/PINA/tutorial6/tutorial.html). Different problems require different domain, here below we summarize the relevant ones:\n",
    "\n",
    "| Problem Type            | Required Domain                |\n",
    "|-------------------------|--------------------------------|\n",
    "| `SpatialProblem`        | `spatial_domain`              |\n",
    "| `TimeDependentProblem`  | `temporal_domain`             |\n",
    "| `ParametricProblem`     | `parameter_domain`            |\n",
    "| `InverseProblem`        | `unknown_parameter_domain`    |\n",
    "\n",
    "\n",
    "As you can see, we implemented the `ode_equation` function which given the model ouput and input returns the equation residual. These residuals are the ones minimized during PINN optimization. \n",
    "\n",
    "**How are the residuals computed?**\n",
    "Given the output we perform differential operation using the [operator modulus](https://mathlab.github.io/PINA/_rst/operator.html). It is pretty intuitive, each differential operator takes the following inputs: \n",
    "- A tensor on which the operator is applied. \n",
    "- A tensor with respect to which the operator is computed. \n",
    "- The names of the output variables for which the operator is evaluated. \n",
    "- The names of the variables with respect to which the operator is computed.\n",
    "We also have a `fast` version of differential operators, where no checks are performed. This can be used to boost performances, once you know the standard ones are doing their job. \n",
    "\n",
    "Notice that we do not pass directly a `python` function, but an `Equation` object, which is initialized with the `python` function. This is done so that all the computations and internal checks are done inside **PINA**, see [the related tutorials](https://mathlab.github.io/PINA/tutorial12/tutorial.html) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606573",
   "metadata": {},
   "source": [
    "#### Generate data for Physical Problems\n",
    "\n",
    "When training physics based models, data can come in form of direct numerical simulation results (tensors, graph), or points in the domains which need to be sampled. In case we perform unsupervised learning, we just need the collocation points for training, i.e. points where we want to evaluate the neural network. Sampling point in **PINA** is very easy. But first, let's check if the domains are dicsretized by using the `are_all_domains_discretised` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_oscillator_problem.are_all_domains_discretised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7022",
   "metadata": {},
   "source": [
    "This is false becase the input points are not available (we need to discretize!). To discretise the problem you can use the `discretise_domain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 20 points in [0, 1] through discretization in all locations\n",
    "harmonic_oscillator_problem.discretise_domain(n=20, mode=\"grid\", domains=\"all\")\n",
    "\n",
    "# sampling 20 points in (0, 1) through latin hypercube sampling in D, and 1 point in x0\n",
    "harmonic_oscillator_problem.discretise_domain(n=20, mode=\"grid\", domains=[\"pde\"])\n",
    "harmonic_oscillator_problem.discretise_domain(n=1, mode=\"random\", domains=[\"initial\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb74a9b",
   "metadata": {},
   "source": [
    "The points are saved in a python `dict`, and can be accessed by calling the attributes input_pts` or `discretised_domains` of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input points:\", harmonic_oscillator_problem.input_pts)\n",
    "for location in harmonic_oscillator_problem.input_pts:\n",
    "    coords = harmonic_oscillator_problem.input_pts[location].flatten()\n",
    "    plt.scatter(coords, torch.zeros_like(coords), s=10, label=location)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f28110",
   "metadata": {},
   "source": [
    "Once the problem is setup, training can be done in less then ten lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed397e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model (2 layers by 64 with GELU activation, default)\n",
    "model = FeedForward(\n",
    "    input_dimensions=1, output_dimensions=1, layers=[64, 64], func=nn.GELU\n",
    ")\n",
    "# build the solver\n",
    "pinn = PINN(harmonic_oscillator_problem, model)\n",
    "# build the trainer and train\n",
    "trainer = Trainer(pinn, max_epochs=1000, accelerator=\"cpu\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we can visualize the results\n",
    "with torch.no_grad():\n",
    "    new_data = harmonic_oscillator_problem.temporal_domain.sample(1000, \"grid\")\n",
    "    true_solution = harmonic_oscillator_problem.solution(new_data)\n",
    "    pinn_solution = pinn(new_data)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(new_data, true_solution, label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(new_data, pinn_solution, \"r\", label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(\n",
    "        new_data, (true_solution - pinn_solution).abs(), \"r\", label=\"Absolute Error\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa365a83",
   "metadata": {},
   "source": [
    "### Benchmarking PINNs variants\n",
    "\n",
    "One of PINA‚Äôs key strengths is its modular design, which makes benchmarking both straightforward and flexible. To illustrate this, we will use the one-dimensional Advection equation. The one-dimensional Advection problem we aim to solve is mathematically defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial t} u(x, t) + c \\frac{\\partial}{\\partial x} u(x, t) = 0 \\quad (x, t) \\in [0,2\\pi]\\times [0,1], \\\\\n",
    "u(x, t) = \\sin(x) \\quad (x, t) \\in [0,2\\pi]\\times {0},\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "While this example is simple and pedagogical, it's important to note that the solution for $c\\gg 1$ posits a great challenge for PINNs. In PINA we have it already implemented in the `problem.zoo` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina.problem.zoo.advection import AdvectionProblem\n",
    "\n",
    "advection_problem = AdvectionProblem(c=5)  # increase c to make it more challenging\n",
    "advection_problem.discretise_domain(\n",
    "    sample_rules={\"x\": {\"n\": 20, \"mode\": \"grid\"}, \"t\": {\"n\": 200, \"mode\": \"grid\"}},\n",
    "    domains=[\"D\"],\n",
    ")\n",
    "advection_problem.discretise_domain(\n",
    "    sample_rules={\"x\": {\"n\": 20, \"mode\": \"grid\"}, \"t\": {\"n\": 1, \"mode\": \"grid\"}},\n",
    "    domains=[\"t0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4e6d1",
   "metadata": {},
   "source": [
    "For benchmarking will use the classical `PINN` solver and its variants, namely `GradientPINN` [5] and `RBAPINN` [6]. We brifely report below how their loss function is defined:\n",
    "\n",
    "**Classical PINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\mathcal{A}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)]) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\mathcal{B}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)])$$\n",
    "\n",
    "**Gradient PINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\mathcal{A}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)]) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\mathcal{L}(\\mathcal{B}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)]) + \\frac{1}{N\\cdot D}\\sum_{i=1}^N\\sum_{d=1}^D \\nabla_{x^{(d)}}\\mathcal{L}(\\mathcal{A}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)])$$\n",
    "\n",
    "**RBAPINN**\n",
    "$$\\theta_{\\rm{best}}=\\min_{\\theta}\\mathcal{L}_{\\rm{problem}}(\\theta), \\quad  \\mathcal{L}_{\\rm{problem}}(\\theta)= \\frac{1}{N_{D}}\\sum_{i=1}^N\n",
    "\\lambda_{\\Omega}^i\\mathcal{L}(\\mathcal{A}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)]) +\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\lambda_{\\partial \\Omega}^i\\mathcal{L}(\\mathcal{B}[\\mathcal{M}_{\\theta}(\\mathbf{x}_i)]), \\quad \\lambda^i_{k+1} \\leftarrow \\gamma\\lambda^i_{k} + \n",
    "        \\eta\\frac{\\lvert r_i\\rvert}{\\max_j \\lvert r_j\\rvert}\\;\\forall k,i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainers = {}\n",
    "model = FeedForward(2, 1, inner_size=32, n_layers=2)\n",
    "for Solver in [RBAPINN]:\n",
    "    solver = Solver(\n",
    "        problem=advection_problem,\n",
    "        model=model,\n",
    "        optimizer=TorchOptimizer(torch.optim.Adam, lr=1e-4),\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"benchmark_ckpt/\",\n",
    "        filename=f\"{Solver.__name__}\",\n",
    "    )\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=\"benchmark_logs/\",\n",
    "        name=f\"{Solver.__name__}\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        solver,\n",
    "        max_epochs=100000,\n",
    "        accelerator=\"cpu\",\n",
    "        enable_model_summary=False,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "    # trainer.train() # <=== Uncomment if you want to train ~30 mins for model\n",
    "    trainers[Solver.__name__] = trainer\n",
    "    model.apply(\n",
    "        lambda layer: (\n",
    "            layer.reset_parameters() if hasattr(layer, \"reset_parameters\") else None\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(pts, predicted, target):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.tricontourf(\n",
    "        pts.extract([\"x\"]).flatten(), pts.extract([\"t\"]).flatten(), predicted\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Neural Network solution\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.tricontourf(pts.extract([\"x\"]).flatten(), pts.extract([\"t\"]).flatten(), target)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"True solution\")\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss_fn = LpLoss(p=2, relative=True)\n",
    "    for idx, Solver in enumerate([PINN, GradientPINN, RBAPINN]):\n",
    "        # get the solver LOAD IT FROM CHECKPOINT <======\n",
    "        solver_name = Solver.__name__\n",
    "        solver = Solver.load_from_checkpoint(\n",
    "            checkpoint_path=f\"benchmark_ckpt/{solver_name}.ckpt\",\n",
    "            problem=advection_problem,\n",
    "            model=model,\n",
    "        )\n",
    "        # compute solution\n",
    "        pts = solver.problem.input_pts[\"D\"]\n",
    "        predicted = solver(pts).extract(\"u\").flatten()\n",
    "        target = solver.problem.solution(pts).flatten()\n",
    "        # sample new test points\n",
    "        print(\n",
    "            f\"L2 relative error {solver_name}:   {loss_fn(target, predicted).item():.5}\"\n",
    "        )\n",
    "# Plot solution only for the last\n",
    "plot_solution(pts, predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96ded0",
   "metadata": {},
   "source": [
    "You can also look at how the loss (and other metrics) decrease, by analyzing the `benchmark_logs`. Just run\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=benchmark_logs\n",
    "```\n",
    "\n",
    "To obtain:\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/dario-coscia/KTH-Summer-School-PINNs-PINA/main/imgs/logs.png\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be501535",
   "metadata": {},
   "source": [
    "### Solving Inverse Problems with PINNs\n",
    "\n",
    "The inverse problem is defined as a Poisson equation with homogeneous boundary conditions with unknown parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\Delta u = e^{-2(x - \\mu_1)^2 - 2(y - \\mu_2)^2} \\quad \\text{in } \\Omega, \\\\\n",
    "u = 0 \\quad \\text{on } \\partial \\Omega, \\\\\n",
    "u(\\mu_1, \\mu_2) = \\text{data}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\Omega$ is the square domain $[-2, 2] \\times [-2, 2]$, and $\\partial \\Omega = \\Gamma_1 \\cup \\Gamma_2 \\cup \\Gamma_3 \\cup \\Gamma_4$ represents the union of its boundaries.\n",
    "\n",
    "This type of setup defines an *inverse problem*, which has two primary objectives:\n",
    "\n",
    "- **Find the solution** $u$ that satisfies the Poisson equation,\n",
    "- **Identify the unknown parameters** $(\\mu_1, \\mu_2)$ that best fit the given data (as described by the third equation in the system).\n",
    "\n",
    "To tackle both objectives, we will define an `InverseProblem` using **PINA**!\n",
    "\n",
    "We import the pre-saved data corresponding to the true parameter values $(\\mu_1, \\mu_2) = (0.5, 0.5)$.  \n",
    "These values represent the *optimal parameters* that we aim to recover through neural network training.\n",
    "\n",
    "In particular, we load:\n",
    "\n",
    "- `input` points ‚Äî the spatial coordinates where observations are available,\n",
    "- `target` points ‚Äî the corresponding $u$ values (i.e., the solution evaluated at the `input` points).\n",
    "\n",
    "This data will be used to guide the inverse problem and supervise the network‚Äôs prediction of the unknown parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = torch.load(\"data/pinn_solution_0.5_0.5\", weights_only=False).detach()\n",
    "data_input = torch.load(\"data/pts_0.5_0.5\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706d13f",
   "metadata": {},
   "source": [
    "Next, we initialize the Poisson problem, which inherits from the `SpatialProblem` and `InverseProblem` classes.  \n",
    "In this step, we need to define all the variables and specify the domain in which our unknown parameters $(\\mu_1, \\mu_2)$ reside.\n",
    "\n",
    "Note that the Laplace equation also takes these unknown parameters as inputs. These parameters will be treated as variables that the neural network will optimize during the training process, enabling it to learn the optimal values for $(\\mu_1, \\mu_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_equation(input_, output_, params_):\n",
    "    force_term = torch.exp(\n",
    "        -2 * (input_.extract([\"x\"]) - params_[\"mu1\"]) ** 2\n",
    "        - 2 * (input_.extract([\"y\"]) - params_[\"mu2\"]) ** 2\n",
    "    )\n",
    "    delta_u = laplacian(output_, input_, components=[\"u\"], d=[\"x\", \"y\"])\n",
    "    return delta_u - force_term\n",
    "\n",
    "\n",
    "class Poisson(SpatialProblem, InverseProblem):\n",
    "\n",
    "    output_variables = [\"u\"]\n",
    "    x_min, x_max = -2, 2\n",
    "    y_min, y_max = -2, 2\n",
    "    spatial_domain = CartesianDomain({\"x\": [x_min, x_max], \"y\": [y_min, y_max]})\n",
    "    unknown_parameter_domain = CartesianDomain({\"mu1\": [-1, 1], \"mu2\": [-1, 1]})\n",
    "\n",
    "    domains = {\n",
    "        \"g1\": CartesianDomain({\"x\": [x_min, x_max], \"y\": y_max}),\n",
    "        \"g2\": CartesianDomain({\"x\": [x_min, x_max], \"y\": y_min}),\n",
    "        \"g3\": CartesianDomain({\"x\": x_max, \"y\": [y_min, y_max]}),\n",
    "        \"g4\": CartesianDomain({\"x\": x_min, \"y\": [y_min, y_max]}),\n",
    "        \"D\": CartesianDomain({\"x\": [x_min, x_max], \"y\": [y_min, y_max]}),\n",
    "    }\n",
    "\n",
    "    conditions = {\n",
    "        \"g1\": Condition(domain=\"g1\", equation=FixedValue(0.0)),\n",
    "        \"g2\": Condition(domain=\"g2\", equation=FixedValue(0.0)),\n",
    "        \"g3\": Condition(domain=\"g3\", equation=FixedValue(0.0)),\n",
    "        \"g4\": Condition(domain=\"g4\", equation=FixedValue(0.0)),\n",
    "        \"D\": Condition(domain=\"D\", equation=Equation(laplace_equation)),\n",
    "        \"data\": Condition(input=data_input, target=data_output),\n",
    "    }\n",
    "\n",
    "\n",
    "problem = Poisson()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79df9b",
   "metadata": {},
   "source": [
    "Next, we define the neural network model that will be used for solving the inverse problem. In this case, we use a simple FeedForeard model, but you could build one that imposes *hard constraints* on the boundary conditions, similar to the approach used in the [Wave tutorial](https://mathlab.github.io/PINA/tutorial3/tutorial.html) to have better performances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForward(\n",
    "    layers=[20, 20, 20],\n",
    "    func=torch.nn.Softplus,\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    input_dimensions=len(problem.input_variables),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9d27e",
   "metadata": {},
   "source": [
    "After that, we discretize the spatial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ddd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.discretise_domain(20, \"grid\", domains=[\"D\"])\n",
    "problem.discretise_domain(\n",
    "    1000,\n",
    "    \"random\",\n",
    "    domains=[\"g1\", \"g2\", \"g3\", \"g4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475b192",
   "metadata": {},
   "source": [
    "Here, we define a simple callback for the trainer. This callback is used to save the parameters predicted by the neural network during training.  \n",
    "The parameters are saved every 100 epochs as `torch` tensors in a specified directory (in our case, `tutorial_logs`).\n",
    "\n",
    "The goal of this setup is to read the saved parameters after training and visualize their trend across the epochs. This allows us to monitor how the predicted parameters evolve throughout the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary directory for saving logs of training\n",
    "tmp_dir = \"tutorial_logs\"\n",
    "\n",
    "\n",
    "class SaveParameters(Callback):\n",
    "    \"\"\"\n",
    "    Callback to save the parameters of the model every 100 epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, __):\n",
    "        if trainer.current_epoch % 100 == 99:\n",
    "            torch.save(\n",
    "                trainer.solver.problem.unknown_parameters,\n",
    "                \"{}/parameters_epoch{}\".format(tmp_dir, trainer.current_epoch),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8be1a",
   "metadata": {},
   "source": [
    "Then, we define the `PINN` object and train the solver using the `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1500\n",
    "pinn = PINN(problem, model, optimizer=TorchOptimizer(torch.optim.Adam, lr=0.005))\n",
    "# define the trainer for the solver\n",
    "trainer = Trainer(\n",
    "    solver=pinn,\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=max_epochs,\n",
    "    default_root_dir=tmp_dir,\n",
    "    enable_model_summary=False,\n",
    "    callbacks=[SaveParameters()],\n",
    "    train_size=1.0,\n",
    "    val_size=0.0,\n",
    "    test_size=0.0,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9e9e3",
   "metadata": {},
   "source": [
    "One can now see how the parameters vary during the training by reading the saved solution and plotting them. The plot shows that the parameters stabilize to their true value before reaching the epoch $1500$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_saved = range(99, max_epochs, 100)\n",
    "parameters = torch.empty((int(max_epochs / 100), 2))\n",
    "for i, epoch in enumerate(epochs_saved):\n",
    "    params_torch = torch.load(\n",
    "        \"{}/parameters_epoch{}\".format(tmp_dir, epoch), weights_only=False\n",
    "    )\n",
    "    for e, var in enumerate(pinn.problem.unknown_variables):\n",
    "        parameters[i, e] = params_torch[var].data\n",
    "\n",
    "# Plot parameters\n",
    "plt.close()\n",
    "plt.plot(epochs_saved, parameters[:, 0], label=\"mu1\", marker=\"o\")\n",
    "plt.plot(epochs_saved, parameters[:, 1], label=\"mu2\", marker=\"s\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Parameter value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e27cd",
   "metadata": {},
   "source": [
    "## üëâ Generative Modelling\n",
    "\n",
    "**Generative models** learn a (conditional) probability distribution using neural network models.  \n",
    "Given a target distribution $p_{\\mathrm{target}}(x)$, the goal of a *fast generative model* is to learn a transformation that allows us to efficiently draw new samples that resemble the target data. Unlike diffusion models, which rely on slow stochastic sampling procedures, fast generative models aim to produce high-quality samples in only a few neural network evaluations‚Äîideally in a single deterministic forward pass.\n",
    "\n",
    "A key example of such an approach is **flow matching** [8]. Flow matching methods learn a *continuous normalizing flow* defined by an ordinary differential equation (ODE)\n",
    "\n",
    "$$\n",
    "\\frac{d x_t}{d t} = v_\\theta(x_t, t),\n",
    "$$\n",
    "\n",
    "where $v_\\theta$ is a neural network that parameterizes a time-dependent velocity field transporting samples from a simple base distribution $p_0$ (e.g., a Gaussian) toward the target distribution $p_{\\mathrm{target}}$.  \n",
    "To train this velocity field, flow matching introduces an analytically defined *target flow* $u_t(x)$ that moves samples along known interpolation paths‚Äîmost commonly the linear flow\n",
    "\n",
    "$$\n",
    "x_t = (1 - t)x_0 + t x_1,\n",
    "\\qquad \n",
    "u_t(x_t \\mid x_0, x_1) = x_1 - x_0,\n",
    "$$\n",
    "\n",
    "where $x_0 \\sim p_0$ and $x_1 \\sim p_{\\mathrm{target}}$.  \n",
    "The learning objective simply matches the model velocity to this target velocity:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "= \\mathbb{E}_{t,\\, x_0,\\, x_1}\n",
    "\\left[ \\left\\| v_\\theta(x_t, t) - u_t(x_t \\mid x_0, x_1) \\right\\|^2 \\right].\n",
    "$$\n",
    "\n",
    "Once trained, generation is extremely fast: we draw $x_0 \\sim p_0$ and solve the ODE forward from $t=0$ to $t=1$, giving a sample distributed as $p_{\\mathrm{target}}$. Because the learned flow is deterministic and smooth, this leads to rapid, stable sampling‚Äîillustrating why flow matching is a powerful tool for building fast generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93561894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeProblem(AbstractProblem):\n",
    "\n",
    "    conditions = {}\n",
    "    input_variables = output_variables = None\n",
    "\n",
    "    def __init__(self, n_points=5000, input_variables=None, output_variables=None):\n",
    "        # Set input and output variables\n",
    "        self.input_variables = input_variables\n",
    "        self.output_variables = output_variables\n",
    "        input_data = self.create_dataset(n_points)\n",
    "        # Set the condition\n",
    "        self.conditions[\"data\"] = DataCondition(input=input_data)\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(batch_size):\n",
    "        x1 = torch.rand(batch_size) * 4 - 2\n",
    "        x2_ = torch.rand(batch_size) - torch.randint(high=2, size=(batch_size,)) * 2\n",
    "        x2 = x2_ + (torch.floor(x1) % 2)\n",
    "        data = 1.0 * torch.cat([x1[:, None], x2[:, None]], dim=1) / 0.45\n",
    "        return data.float()\n",
    "\n",
    "# let's see some samples\n",
    "data = GenerativeProblem.create_dataset(5000)\n",
    "fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "H = axs.hist2d(\n",
    "    data[:, 0], data[:, 1], 300, range=((-5, 5), (-5, 5))\n",
    ")\n",
    "cmin = 0.0\n",
    "cmax = torch.quantile(torch.from_numpy(H[0]), 0.99).item()\n",
    "norm = cm.colors.Normalize(vmax=cmax, vmin=cmin)\n",
    "_ = axs.hist2d(\n",
    "    data[:, 0],\n",
    "    data[:, 1],\n",
    "    300,\n",
    "    range=((-5, 5), (-5, 5)),\n",
    "    norm=norm,\n",
    ")\n",
    "axs.set_aspect(\"equal\")\n",
    "axs.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872717eb",
   "metadata": {},
   "source": [
    "Let's now create the solver and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingSolver(SingleSolverInterface):\n",
    "    # this is needed when defying the optimization loop\n",
    "    accepted_conditions_types = DataCondition\n",
    "\n",
    "    # FM optimization\n",
    "    def optimization_cycle(self, batch):\n",
    "        condition_loss = {}\n",
    "        for condition_name, points in batch:\n",
    "            # set final time to input point\n",
    "            x_1 = points[\"input\"]\n",
    "            # sample time\n",
    "            t = torch.rand((x_1.shape[0], 1))\n",
    "            # sample initial state\n",
    "            x_0 = torch.randn_like(x_1)\n",
    "            # compute x_t\n",
    "            x_t = t * x_1 + (1 - t) * x_0\n",
    "            # compute true field\n",
    "            u_t = x_1 - x_0\n",
    "            # compute ML field\n",
    "            input_ = {\"data\": x_t, \"time\": t}\n",
    "            v_t = self.forward(input_)\n",
    "            condition_loss[condition_name] = torch.nn.functional.mse_loss(v_t, u_t)\n",
    "        return condition_loss\n",
    "\n",
    "    # sampling\n",
    "    @torch.no_grad()\n",
    "    def sample(self, data_size, integration_steps):\n",
    "        # sample initial gaussian\n",
    "        x_0 = torch.randn(size=data_size)\n",
    "        # do euler integration (or any other)\n",
    "        traj = [x_0]\n",
    "        times = [0]\n",
    "        for t in range(1, integration_steps):\n",
    "            input_ = {\n",
    "                \"data\": traj[-1],\n",
    "                \"time\": torch.tensor(times[-1]),\n",
    "            }\n",
    "            x_t = traj[-1] + (1 / integration_steps) * self.forward(input_)\n",
    "            traj.append(x_t)\n",
    "            times.append((t + 1) / integration_steps)\n",
    "\n",
    "        return traj, times\n",
    "\n",
    "\n",
    "class VectorFieldModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 2\n",
    "        self.time_dim = 1\n",
    "        self.mlp = FeedForward(\n",
    "            input_dimensions=3,\n",
    "            output_dimensions=2,\n",
    "            n_layers=3,\n",
    "            inner_size=128,\n",
    "            func=torch.nn.Tanh\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # extract input\n",
    "        x = input[\"data\"]\n",
    "        t = input[\"time\"]\n",
    "        # reshaping\n",
    "        sz = x.size()\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        h = torch.cat([x, t], dim=1)\n",
    "        return self.mlp(h).reshape(*sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d71d84",
   "metadata": {},
   "source": [
    "Then we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    solver=FlowMatchingSolver(\n",
    "        problem=GenerativeProblem(5000), model=VectorFieldModel(), use_lt=False\n",
    "    ),\n",
    "    max_epochs=2000,\n",
    "    batch_size=None,\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "\n",
    "# now train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a4d99",
   "metadata": {},
   "source": [
    "Let's sample from the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_steps = 10\n",
    "traj, times = trainer.solver.sample(\n",
    "    data_size=(100000, 2),\n",
    "    integration_steps=integration_steps,\n",
    ")\n",
    "from matplotlib import cm\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(16, 8))\n",
    "idx = 0\n",
    "for i in range(0, len(times), len(times)//9):\n",
    "    H = axs[idx].hist2d(traj[i][:, 0], traj[i][:, 1], 300, range=((-5, 5), (-5, 5)))\n",
    "    cmin = 0.0\n",
    "    cmax = torch.quantile(torch.from_numpy(H[0]), 0.99).item()\n",
    "    norm = cm.colors.Normalize(vmax=cmax, vmin=cmin)\n",
    "    _ = axs[idx].hist2d(\n",
    "        traj[i][:, 0],\n",
    "        traj[i][:, 1],\n",
    "        300,\n",
    "        range=((-5, 5), (-5, 5)),\n",
    "        norm=norm,\n",
    "    )\n",
    "    axs[idx].set_aspect(\"equal\")\n",
    "    axs[idx].axis(\"off\")\n",
    "    axs[idx].set_title(\"t= %.2f\" % times[i])\n",
    "    idx += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fa20f",
   "metadata": {},
   "source": [
    "## üëâ References\n",
    "\n",
    "1. Coscia, D., Ivagnes, A., Demo, N., & Rozza, G. (2023), *Physics-Informed Neural networks for Advanced modeling.*, Journal of Open Source Software, 8(87), 5352.\n",
    "2. Rozza G., Stabile G., Ballarin F. (2022). Advanced Reduced Order Methods and Applications in Computational Fluid Dynamics, Society for Industrial and Applied Mathematics. \n",
    "3. Hesthaven, J. S., & Ubbiali, S. (2018). Non-intrusive reduced order modeling of nonlinear problems using neural networks. Journal of Computational Physics, 363, 55-78.\n",
    "4. Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707.\n",
    "5. Yu, J., Lu, L., Meng, X., & Karniadakis, G. E. (2022). Gradient-enhanced physics-informed neural networks for forward and inverse PDE problems. Computer Methods in Applied Mechanics and Engineering, 393, 114823.\n",
    "6. Anagnostopoulos, S. J., Toscano, J. D., Stergiopulos, N., & Karniadakis, G. E. (2023). Residual-based attention and connection to information bottleneck theory in PINNs. arXiv preprint arXiv:2307.00379.\n",
    "7. Unke, O. T., Chmiela, S., Sauceda, H. E., Gastegger, M., Poltavsky, I., Schutt, K. T., ... & MuÃàller, K. R. (2021). Machine learning force fields. Chemical Reviews, 121(16), 10142-10186.\n",
    "8. Lipman, Yaron, et al. \"Flow matching for generative modeling.\" arXiv preprint arXiv:2210.02747 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e2cf3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
